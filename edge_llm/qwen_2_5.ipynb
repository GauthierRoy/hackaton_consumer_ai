{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  \"Incorrect: 'la soleil'; Correct: 'le soleil'\"\n",
      "]\n",
      "```\n",
      "```json\n",
      "[\n",
      "  \"Incorrect: 'la soleil'; Correct: 'le soleil'\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import json\n",
    "response: ChatResponse = chat(model='qwen2.5:3b', messages=[\n",
    "  {\n",
    "    'role': 'system',\n",
    "    'content': \"\"\"\n",
    "<system_prompt>\n",
    "YOU ARE THE MOST ACCURATE AND METICULOUS FRENCH LANGUAGE EXPERT, RECOGNIZED INTERNATIONALLY FOR YOUR ABILITY TO SPOT SPELLING AND GRAMMATICAL MISTAKES. YOUR TASK IS TO REVIEW THE PROVIDED TEXT AND IDENTIFY ALL MISTAKES WITH UNPARALLELED PRECISION, OFFERING THE CORRECT FORMS WHERE APPLICABLE. YOUR GOAL IS TO ENSURE THAT THE FINAL TEXT IS FLAWLESS IN TERMS OF FRENCH GRAMMAR, SPELLING, AND PUNCTUATION.\n",
    "\n",
    "###INSTRUCTIONS###\n",
    "\n",
    "1. ANALYZE THE TEXT METICULOUSLY:\n",
    "   1.1 READ the provided text carefully and thoroughly.\n",
    "   1.2 IDENTIFY spelling, grammatical, and syntactical mistakes with precision.\n",
    "   1.3 DETECT issues with accents, agreements (gender, number, tense), punctuation, and word order.\n",
    "\n",
    "2. DOCUMENT EACH MISTAKE CLEARLY:\n",
    "   2.1 For each identified mistake, provide the incorrect form in the exact text.\n",
    "   2.2 Suggest the correct form.\n",
    "   2.3 Group mistakes into a JSON array following the specified output format.\n",
    "\n",
    "3. USE THE FOLLOWING OUTPUT FORMAT:\n",
    "   - Each mistake should be captured as a string indicating the error, with the corrected form appended (e.g., `\"Incorrect: 'se sont lever'; Correct: 'se sont levés'\"`).\n",
    "   - Each mistake must be recorded in a JSON array.\n",
    "\n",
    "4. ENSURE THE OUTPUT FORMAT STRICTLY MATCHES:\n",
    "   ```json\n",
    "   [\n",
    "     \"Incorrect: '...', Correct: '...'\",\n",
    "     \"Incorrect: '...', Correct: '...'\"\n",
    "   ]\n",
    "```  \n",
    "\"\"\",\n",
    "  },\n",
    "  {\n",
    "      \"role\":\"user\",\n",
    "      \"content\":\"J'aimes la soleil\"\n",
    "  }\n",
    "])\n",
    "mistake = json\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import json\n",
    "response: ChatResponse = chat(model='qwen2.5:3b', messages=[\n",
    "  {\n",
    "    'role': 'system',\n",
    "    'content': \"\"\"\n",
    "<system_prompt>\n",
    "YOU ARE THE MOST ACCURATE AND METICULOUS FRENCH LANGUAGE EXPERT, RECOGNIZED INTERNATIONALLY FOR YOUR ABILITY TO SPOT SPELLING AND GRAMMATICAL MISTAKES. YOUR TASK IS TO REVIEW THE PROVIDED TEXT AND IDENTIFY ALL MISTAKES WITH UNPARALLELED PRECISION, OFFERING THE CORRECT FORMS WHERE APPLICABLE. YOUR GOAL IS TO ENSURE THAT THE FINAL TEXT IS FLAWLESS IN TERMS OF FRENCH GRAMMAR, SPELLING, AND PUNCTUATION.\n",
    "\n",
    "###INSTRUCTIONS###\n",
    "\n",
    "1. ANALYZE THE TEXT METICULOUSLY:\n",
    "   1.1 READ the provided text carefully and thoroughly.\n",
    "   1.2 IDENTIFY spelling, grammatical, and syntactical mistakes with precision.\n",
    "   1.3 DETECT issues with accents, agreements (gender, number, tense), punctuation, and word order.\n",
    "\n",
    "2. DOCUMENT EACH MISTAKE CLEARLY:\n",
    "   2.1 For each identified mistake, provide the incorrect form in the exact text.\n",
    "   2.2 Suggest the correct form.\n",
    "   2.3 Group mistakes into a JSON array following the specified output format.\n",
    "\n",
    "3. USE THE FOLLOWING OUTPUT FORMAT:\n",
    "   - Each mistake should be captured as a string indicating the error, with the corrected form appended (e.g., `\"Incorrect: 'se sont lever'; Correct: 'se sont levés'\"`).\n",
    "   - Each mistake must be recorded in a JSON array.\n",
    "\n",
    "4. ENSURE THE OUTPUT FORMAT STRICTLY MATCHES:\n",
    "   ```json\n",
    "   [\n",
    "     \"Incorrect: '...', Correct: '...'\",\n",
    "     \"Incorrect: '...', Correct: '...'\"\n",
    "   ]\n",
    "```  \n",
    "\"\"\",\n",
    "  },\n",
    "  {\n",
    "      \"role\":\"user\",\n",
    "      \"content\":\"J'aimes la soleil\"\n",
    "  }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import json\n",
    "response: ChatResponse = chat(model='qwen2.5:3b', messages=[\n",
    "  {\n",
    "    'role': 'system',\n",
    "    'content': \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygrammalecte import grammalecte_text,GrammalecteGrammarMessage\n",
    "\n",
    "texte_bidon = \"\"\"\\\n",
    "J'aimes la Soleil\n",
    "\"\"\"\n",
    "class DetectLangageMistakes:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mistakes_history = []\n",
    "\n",
    "    def _transform_grammarlect_error(self,text:str,messages:list[GrammalecteGrammarMessage]) -> dict:\n",
    "        errors = []\n",
    "        texts = text.splitlines()\n",
    "        for message in messages:\n",
    "            print(message.line)\n",
    "            print(message.start)\n",
    "            print(message.end)\n",
    "            formatted_error = {}\n",
    "            formatted_error[\"word_error\"] = texts[message.line-1][message.start:message.end]\n",
    "            formatted_error[\"rule\"] = message.message\n",
    "            if message.suggestions:\n",
    "                formatted_error[\"suggestion\"] = message.suggestions\n",
    "            errors.append(\n",
    "                formatted_error\n",
    "            )\n",
    "        return errors\n",
    "        \n",
    "    \n",
    "    def spot_mistake(self,text:str)->dict:\n",
    "        mistakes = grammalecte_text(text)\n",
    "        if not mistakes:\n",
    "            return False\n",
    "        else:\n",
    "            mistakes_formatted = self._transform_grammarlect_error(text,mistakes)\n",
    "        self.mistakes_history.extend(mistakes_formatted)\n",
    "        return mistakes_formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_langage_mistakes = DetectLangageMistakes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GrammalecteSpellingMessage' object has no attribute 'suggestions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdetect_langage_mistakes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspot_mistake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBonjour ca va bien la viee ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 33\u001b[0m, in \u001b[0;36mDetectLangageMistakes.spot_mistake\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     mistakes_formatted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_grammarlect_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmistakes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmistakes_history\u001b[38;5;241m.\u001b[39mextend(mistakes_formatted)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mistakes_formatted\n",
      "Cell \u001b[0;32mIn[69], line 21\u001b[0m, in \u001b[0;36mDetectLangageMistakes._transform_grammarlect_error\u001b[0;34m(self, text, messages)\u001b[0m\n\u001b[1;32m     19\u001b[0m formatted_error[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m texts[message\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][message\u001b[38;5;241m.\u001b[39mstart:message\u001b[38;5;241m.\u001b[39mend]\n\u001b[1;32m     20\u001b[0m formatted_error[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrule\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggestions\u001b[49m:\n\u001b[1;32m     22\u001b[0m     formatted_error[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39msuggestions\n\u001b[1;32m     23\u001b[0m errors\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     24\u001b[0m     formatted_error\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GrammalecteSpellingMessage' object has no attribute 'suggestions'"
     ]
    }
   ],
   "source": [
    "detect_langage_mistakes.spot_mistake(\"Bonjour ca va bien la viee ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte_bidon = \"\"\"J'aime les soleil.\n",
    "Voituree\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voituree\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"J'aime les soleil.\n",
    "Voituree\"\"\"\n",
    "\n",
    "# Step 1: Split the text into lines\n",
    "lines = text.splitlines()\n",
    "\n",
    "# Step 2: Extract the desired line (line index 1 for line=2)\n",
    "line_of_interest = lines[1]\n",
    "\n",
    "# Step 3: Extract the substring from start=0 to end=8\n",
    "substring = line_of_interest[0:8]\n",
    "\n",
    "print(substring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GrammalecteGrammarMessage(line=1, start=11, end=17, url='', color=[64, 127, 191], suggestions=['soleils'], message='Accord de nombre erroné\\xa0: «\\xa0soleil\\xa0» devrait être au pluriel.', rule='g3__gn_les_1m__b2_a1_1', type='gn'),\n",
       " GrammalecteSpellingMessage(line=2, start=0, end=8, word='Voituree', message='Mot inconnu\\xa0: Voituree')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o for o in grammalecte_text(texte_bidon)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [o for o in grammalecte_text(texte_bidon)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GrammalecteGrammarMessage(line=1, start=11, end=17, url='', color=[64, 127, 191], suggestions=['soleils'], message='Accord de nombre erroné\\xa0: «\\xa0soleil\\xa0» devrait être au pluriel.', rule='g3__gn_les_1m__b2_a1_1', type='gn')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "response: ChatResponse = chat(model='qwen2.5:3b', messages=[\n",
    "  {\n",
    "    'role': 'system',\n",
    "    'content': \"\"\"#ROLE \n",
    "You are privacy expert, spot every sensitive or personnal data by their generic type. \n",
    "# OUPUT FORMAT\n",
    "[\n",
    "  {\"private_data\":\"Tristan\", \"generic_replacement\":\"name\"}\n",
    "]\n",
    "\"\"\",\n",
    "  },\n",
    "  {\n",
    "      \"role\":\"user\",\n",
    "      \"content\":\"J'aime Quentin dont le numéro est le 0678403769\"\n",
    "  }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"private_data\":\"Quentin\", \"generic_replacement\":\"name\"}, {\"private_data\":\"0678403769\", \"generic_replacement\":\"phone_number\"}]\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
